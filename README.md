# ğŸŒ COVID-19 Analytics Platform

![Project Status](https://img.shields.io/badge/status-active-brightgreen)
![Python Version](https://img.shields.io/badge/python-3.9%2B-blue)
![Last Updated](https://img.shields.io/badge/last%20updated-March%202025-blueviolet)

> An end-to-end data pipeline and interactive dashboard for COVID-19 global analytics, combining Airflow for automated data processing and Streamlit for visualization.

![COVID-19 Dashboard Preview](https://github.com/FaheemKhan0817/covid19-dashboard/raw/main/screenshots/dashboard_preview.png)

## ğŸ“‹ Table of Contents
- [Overview](#-overview)
- [Features](#-features)
- [Architecture](#-architecture)
- [Installation](#-installation)
- [Running the Pipeline](#-running-the-pipeline)
- [Using the Dashboard](#-using-the-dashboard)
- [Data Sources](#-data-sources)
- [Deployment](#-deployment)
- [Contributing](#-contributing)
- [License](#-license)
- [Acknowledgements](#-acknowledgements)

## ğŸ” Overview

This project provides a comprehensive solution for COVID-19 data analysis through:

1. **Automated Data Pipeline**: Airflow DAGs that fetch, process, and clean COVID-19 data from authoritative sources
2. **Interactive Dashboard**: A Streamlit web application for visualizing trends, comparing regions, and exploring patterns

Built with data engineering best practices, the platform provides up-to-date analytics with minimal maintenance.

## âœ¨ Features

### Data Pipeline Features
- Automated daily data ingestion from Johns Hopkins CSSE repository
- Robust error handling and retry mechanisms
- Data validation and cleaning steps
- Processed data storage with version tracking
- Email notifications on pipeline failures (configurable)

### Dashboard Features
- Global and country-specific COVID-19 statistics
- Multiple visualization types (line, bar, pie charts)
- Time series analysis of cases, deaths, and recoveries
- Regional and provincial breakdown
- Data exploration and filtering tools
- CSV/Excel export functionality
- Mobile-responsive design

COVID-19 Analytics Platform
â”œâ”€â”€ ğŸ”„ Data Processing Layer (Airflow)
â”‚   â”œâ”€â”€ Data Ingestion DAG
â”‚   â”‚   â””â”€â”€ Fetches raw data from JHU CSSE GitHub
â”‚   â””â”€â”€ Data Processing DAG
â”‚       â””â”€â”€ Cleans, transforms, and stores processed data
â””â”€â”€ ğŸ“Š Visualization Layer (Streamlit)
â”œâ”€â”€ Dashboard Application
â”‚   â”œâ”€â”€ Global statistics
â”‚   â”œâ”€â”€ Country-specific analysis
â”‚   â””â”€â”€ Interactive visualizations
â””â”€â”€ User Interface
â”œâ”€â”€ Filters and controls
â”œâ”€â”€ Multi-tab visualizations
â””â”€â”€ Export functionality
text

---

## ğŸ“¥ Installation

### Prerequisites
- ğŸ Python 3.9+
- ğŸŒ¬ï¸ Apache Airflow 2.0+
- ğŸ“¦ Git

### Setup Instructions

1. **Clone the Repository**
   ```bash
   git clone https://github.com/FaheemKhan0817/covid19-dashboard.git
   cd covid19-dashboard

    Create a Virtual Environment
    bash

python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
Install Dependencies
bash
pip install -r requirements.txt
Create Required Directories
bash
mkdir -p data/raw data/processed
Project Structure
text

    covid19-dashboard/
    â”œâ”€â”€ airflow/
    â”‚   â””â”€â”€ dags/
    â”‚       â”œâ”€â”€ covid_data_pipeline_dag.py
    â”‚       â””â”€â”€ data_quality_checks.py
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ processed/
    â”‚   â””â”€â”€ raw/
    â”œâ”€â”€ dashboard.py
    â”œâ”€â”€ requirements.txt
    â”œâ”€â”€ .streamlit/
    â”‚   â””â”€â”€ config.toml
    â”œâ”€â”€ README.md
    â””â”€â”€ screenshots/
        â”œâ”€â”€ dashboard_preview.png
        â””â”€â”€ airflow_dag_view.png

ğŸ”„ Running the Pipeline
Setting Up Airflow

    Set AIRFLOW_HOME Environment Variable
    bash

export AIRFLOW_HOME=$(pwd)/airflow  # On Windows: set AIRFLOW_HOME=%cd%\airflow
Initialize Airflow Database
bash
airflow db init
Create an Admin User
bash
airflow users create \
    --username admin \
    --firstname Admin \
    --lastname User \
    --role Admin \
    --email admin@example.com \
    --password admin
Copy DAG Files
bash

    cp covid_data_pipeline_dag.py $AIRFLOW_HOME/dags/

Starting Airflow Services

    Start the Scheduler
    bash

airflow scheduler
Start the Webserver (in a new terminal)
bash

    export AIRFLOW_HOME=$(pwd)/airflow
    airflow webserver --port 8080
    Access the Airflow UI
        Open http://localhost:8080
        Log in with your admin credentials
    Run the DAG
        Enable and trigger the covid19_data_pipeline DAG in the UI
        Monitor execution progress

Understanding the DAG

    Data Ingestion Group: Downloads and validates raw data, stores it as CSV.
    Data Processing Group: Cleans, transforms, and prepares data for the dashboard.

ğŸ“Š Using the Dashboard
Running the Streamlit Dashboard
bash
streamlit run dashboard.py

    Access at: http://localhost:8501

Dashboard Navigation

    Cases Over Time: Line/bar charts of cumulative stats
    Daily New Cases: Trends in new infections
    Case Distribution: Pie charts for case proportions
    Regional Data: Detailed provincial/state breakdowns

Dashboard Features

    ğŸŒ Country selection dropdown
    ğŸ“… Custom date range filtering
    ğŸ¨ Toggleable chart types and logarithmic scaling
    ğŸ“¤ Data export to CSV
    ğŸ“‹ Summary stats at a glance

ğŸ“ˆ Data Sources

Sourced from the Johns Hopkins University CSSE COVID-19 Dataset, aggregating data from:

    ğŸŒ World Health Organization (WHO)
    ğŸ‡ºğŸ‡¸ US CDC
    ğŸ‡ªğŸ‡º European CDC
    Various national health departments

Includes: Confirmed cases, deaths, recoveries, and geographic data, updated daily.
ğŸš€ Deployment
Deploying on Streamlit Cloud

    Push to GitHub
    bash

    git add .
    git commit -m "Prepare for Streamlit Cloud deployment"
    git push
    Deploy via Streamlit Cloud
        Log in with GitHub
        Select repository and branch
        Set dashboard.py as the main file
        Click "Deploy"

Deploying Airflow on a Server

    Use Docker Compose, Amazon MWAA, Google Cloud Composer, or Kubernetes.

ğŸ¤ Contributing

We welcome contributions! Follow these steps:

    Fork the repo
    Create a feature branch (git checkout -b feature/amazing-feature)
    Commit changes (git commit -m 'Add some amazing feature')
    Push to the branch (git push origin feature/amazing-feature)
    Open a Pull Request

ğŸ“„ License

Licensed under the MIT License. See the LICENSE file for details.
ğŸ™ Acknowledgements

    Johns Hopkins CSSE: For the COVID-19 dataset
    Streamlit: For an amazing visualization framework
    Apache Airflow: For workflow orchestration
    Plotly: For interactive charts

<p align="center"> <i>Developed with â¤ï¸ by <a href="https://github.com/FaheemKhan0817">FaheemKhan0817</a></i><br> <i>Last updated: 2025-03-03 09:15:47 UTC</i> </p> ```